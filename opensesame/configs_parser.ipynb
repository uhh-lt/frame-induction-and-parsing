{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test opensesame models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**module:** lexsub.run_parser\n",
    "```\n",
    "python -m lexsub.run_parser --configs $JSON_CONFIGS_PATH --workers 12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create json configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "example_stats=\"\"\"verbs\t5739\n",
    "01ExPerSent_verbs_rand01\t2746\n",
    "010pc_verbs\t574\n",
    "020pc_verbs\t1148\n",
    "030pc_verbs\t1722\n",
    "040pc_verbs\t2296\n",
    "050pc_verbs\t2870\n",
    "060pc_verbs\t3443\n",
    "070pc_verbs\t4017\n",
    "080pc_verbs\t4591\n",
    "090pc_verbs\t5165\n",
    "100pc_verbs\t5739\n",
    "nouns\t9293\n",
    "01ExPerSent_nouns_rand01\t2996\n",
    "010pc_nouns\t929\n",
    "020pc_nouns\t1859\n",
    "030pc_nouns\t2788\n",
    "040pc_nouns\t3717\n",
    "050pc_nouns\t4646\n",
    "060pc_nouns\t5576\n",
    "070pc_nouns\t6505\n",
    "080pc_nouns\t7434\n",
    "090pc_nouns\t8364\n",
    "100pc_nouns\t9293\"\"\".split(\"\\n\")\n",
    "\n",
    "examples_per_dataset = {line.split(\"\\t\")[0]:int(line.split(\"\\t\")[1]) for line in example_stats}\n",
    "print(len(examples_per_dataset))\n",
    "# examples_per_dataset\n",
    "EPOCHS = {\n",
    "'TARGETID': 100,\n",
    "'FRAMEID': 100,\n",
    "'ARGID': 10,\n",
    "}\n",
    "NUM_STEPS = {}\n",
    "\n",
    "for ds, size in examples_per_dataset.items():\n",
    "    NUM_STEPS[ds] = {'TARGETID': EPOCHS['TARGETID']*size,\n",
    "                    'FRAMEID': EPOCHS['FRAMEID']*size,\n",
    "                    'ARGID': EPOCHS['ARGID']*size\n",
    "                    }\n",
    "\n",
    "# NUM_STEPS\n",
    "\n",
    "def seed_expName(name):\n",
    "    exp = name.split(\"/\")[0]\n",
    "    exp = exp.split(\"_expanded\")[0]\n",
    "    return exp\n",
    "\n",
    "# [seed_expName(exp) for exp in exps]\n",
    "# NUM_STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dir = 'nExPerSent_verbs_randAllExps'\n",
    "exps = [\n",
    "(exps_dir, '01ExPerSent_verbs_rand01'),\n",
    "]\n",
    "    \n",
    "\n",
    "# exps_dir = 'nPc_verbs_randAllExps'\n",
    "# exps = [\n",
    "# (exps_dir, '010pc_verbs'),\n",
    "# (exps_dir, '020pc_verbs'),\n",
    "# (exps_dir, '030pc_verbs'),\n",
    "# (exps_dir, '040pc_verbs'),\n",
    "# (exps_dir, '050pc_verbs'),\n",
    "# (exps_dir, '100pc_verbs'),    \n",
    "# ]\n",
    "\n",
    "# exps_dir = 'nExPerSent_nouns_randAllExps'\n",
    "# exps = [\n",
    "# ('nExPerSent_nouns_randAllExps', '01ExPerSent_nouns_rand01'),\n",
    "# ]\n",
    "\n",
    "# exps_dir = 'nPc_nouns_randAllExps'\n",
    "# exps = [\n",
    "# ('nPc_nouns_randAllExps', '010pc_nouns'),\n",
    "# ('nPc_nouns_randAllExps', '020pc_nouns'),\n",
    "# ('nPc_nouns_randAllExps', '030pc_nouns'),\n",
    "# ('nPc_nouns_randAllExps', '040pc_nouns'),\n",
    "# ('nPc_nouns_randAllExps', '050pc_nouns'),\n",
    "# ('nPc_nouns_randAllExps', '100pc_nouns'),    \n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "json_file = f'seed_experiments_{exps_dir}'\n",
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dir = \"expanded_nExPerSent_verbs_randAllExps\"\n",
    "expanded_exps = [\n",
    "# '01ExPerSent_verbs_rand01_expanded_lu',\n",
    " '01ExPerSent_verbs_rand01_expanded_roles',\n",
    " '01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-10pc',\n",
    " '01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-30pc',\n",
    " '01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-50pc',\n",
    "#  '01ExPerSent_verbs_rand01_expanded_nouns-10pc',\n",
    "#  '01ExPerSent_verbs_rand01_expanded_nouns-30pc',\n",
    "#  '01ExPerSent_verbs_rand01_expanded_nouns-50pc',\n",
    "]\n",
    "##### --------------------------------------------------\n",
    "# exps_dir = \"expanded_nExPerSent_nouns_randAllExps\"\n",
    "# expanded_exps = [\n",
    "# '01ExPerSent_nouns_rand01_expanded_lu',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_roles',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_lu_roles_nouns-10pc',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_lu_roles_nouns-30pc',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_lu_roles_nouns-50pc',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_nouns-10pc',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_nouns-30pc',\n",
    "#  '01ExPerSent_nouns_rand01_expanded_nouns-50pc',\n",
    "# ]\n",
    "\n",
    "\n",
    "# exps_dir = \"expanded_nPc_verbs_randAllExps\"\n",
    "# exps = [\n",
    "# 'verbs_expanded_nouns-30pc',\n",
    "# 'verbs_expanded_nouns-50pc',\n",
    "# 'verbs_expanded_lu_roles_nouns-30pc',\n",
    "# 'verbs_expanded_lu_roles_nouns-50pc',\n",
    "# ]\n",
    "\n",
    "\n",
    "# exps_dir = \"expanded_nPc_nouns_randAllExps\"\n",
    "# exps = [\n",
    "# 'nouns_expanded_nouns-30pc',\n",
    "# 'nouns_expanded_nouns-50pc',\n",
    "# 'nouns_expanded_lu_roles_nouns-30pc',\n",
    "# 'nouns_expanded_lu_roles_nouns-50pc',\n",
    "# ]\n",
    "\n",
    "# N = [10, 20, 30, 40, 50, 100]\n",
    "# expanded_exps = [f'{i:03d}pc_{exp}' for exp in exps for i in N]\n",
    "# print(len(expanded_exps))\n",
    "# -------------------------------------------------------\n",
    "exps =  []\n",
    "\n",
    "parser='nltk'\n",
    "gold_prefix = 'lugold_rolegold_'\n",
    "# proc_funcs = 'nolemma'\n",
    "proc_funcs = 'nolemma_role_stopwords'\n",
    "\n",
    "\n",
    "pipeline = f'{gold_prefix}{parser}_{proc_funcs}_N2'\n",
    "\n",
    "preds_model = 'xlnet_embs_hypers'\n",
    "\n",
    "exps1 = [f'{exp}/{preds_model}/{pipeline}' for exp in expanded_exps]\n",
    "exps.extend(exps1)\n",
    "\n",
    "preds_model= 'bert'\n",
    "exps2 = [f'{exp}/{preds_model}/{pipeline}' for exp in expanded_exps]\n",
    "exps.extend(exps2)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "exps = [(exps_dir, exp) for exp in exps] \n",
    "print(len(exps))\n",
    "print(exps[0])\n",
    "\n",
    "# # ------------------------------------\n",
    "    \n",
    "json_file = f'{exps_dir}_{pipeline}'\n",
    "json_file    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_output = 'configurations/configs_parser'\n",
    "data_dir = 'data/open_sesame_v1_data/fn1.7'\n",
    "\n",
    "output_dir = '~/parser_workdir/step_logs'\n",
    "\n",
    "# -------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=1\n",
    "start, end = 1, 11\n",
    "# start, end = 6, 11\n",
    "\n",
    "E = [i for i in range(start, end)]\n",
    "\n",
    "\n",
    "if E == [0]: fixseed = True\n",
    "else: fixseed = False\n",
    "if not fixseed:\n",
    "    out_file = f'{json_file}_multirun'\n",
    "else: \n",
    "    out_file = json_file\n",
    "   \n",
    "out_file = f'argid_{out_file}_{M}'\n",
    "# ------------------------------\n",
    "exp_configs= []\n",
    "\n",
    "# MODELS = ['targetid', 'frameid', 'argid']\n",
    "MODELS = ['argid']\n",
    "\n",
    "\n",
    "for exps_dir, e, in exps:\n",
    "    ds = seed_expName(e)\n",
    "#         print(ds, examples_per_dataset[ds])\n",
    "    for i in E:\n",
    "\n",
    "        for model in MODELS:\n",
    "            exp_configs.append({\"name\":f'{e}-{model}-{i:02d}',\n",
    "                              \"args\":{\n",
    "                                    \"model_id\":model,\n",
    "                                    \"model_name\":f'{model}_steps-{NUM_STEPS[ds][model.upper()]}_run-{i:02d}', \n",
    "                                    \"exp_name\":f'{os.path.join(exps_dir, e)}',\n",
    "                                    \"data_dir\":f'{data_dir}',\n",
    "                                    \"output_dir\":f'{output_dir}',\n",
    "                                    \"fixseed\":fixseed,\n",
    "                                     \"num_steps\":NUM_STEPS[ds][model.upper()]\n",
    "                                      }\n",
    "                             })\n",
    "\n",
    "\n",
    "print(f'json_file: {out_file}')    \n",
    "\n",
    "print(len(exp_configs))\n",
    "exp_names =[exp['name'] for exp in exp_configs]\n",
    "# print(','.join(exp_names))\n",
    "\n",
    "\n",
    "with open(f'{configs_output}/{out_file}.json', 'w') as fp:\n",
    "        json.dump(exp_configs, fp, indent=4)\n",
    " \n",
    "exp_configs[0]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
