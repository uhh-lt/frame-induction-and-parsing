{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub.results_parser import parse_results, ttest_df, ttest_df\n",
    "from lexsub.results_parser import compute_statistical_measures,  compute_statistical_measures_lr\n",
    "from lexsub.results_parser import get_results_opensesame, prettify, prettify2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-radio",
   "metadata": {},
   "source": [
    "### 1SentencePerAnnotation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-bread",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_exps_dir = \"nExPerSent_verbs_randAllExps\"\n",
    "seed_exp = [(f\"{seed_exps_dir}/01ExPerSent_verbs_rand01\")]\n",
    "\n",
    "\n",
    "expanded_exps_dir='expanded_nExPerSent_verbs_randAllExps'\n",
    "\n",
    "expanded_exps1 = [\n",
    "'01ExPerSent_verbs_rand01_expanded_lu',\n",
    "'01ExPerSent_verbs_rand01_expanded_nouns-10pc',\n",
    "'01ExPerSent_verbs_rand01_expanded_nouns-30pc',\n",
    "'01ExPerSent_verbs_rand01_expanded_nouns-50pc',\n",
    "]\n",
    "\n",
    "expanded_exps2 = [\n",
    "'01ExPerSent_verbs_rand01_expanded_roles',\n",
    "'01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-10pc',\n",
    "'01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-30pc',\n",
    "'01ExPerSent_verbs_rand01_expanded_lu_roles_nouns-50pc',\n",
    "]\n",
    "\n",
    "pipeline1 = 'lugold_rolegold_nltk_nolemma_N2'\n",
    "pipeline2 = 'lugold_rolegold_nltk_nolemma_role_stopwords_N2'\n",
    "\n",
    "\n",
    "preds_model = 'xlnet_embs_hypers'\n",
    "exps11 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps12 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps11.extend(exps12)\n",
    "\n",
    "preds_model = 'bert'\n",
    "exps21 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps22 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps21.extend(exps22)\n",
    "\n",
    "\n",
    "exps = seed_exp\n",
    "exps.extend(exps11)\n",
    "exps.extend(exps21)\n",
    "\n",
    "\n",
    "all_exps = exps\n",
    "print(len(all_exps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = '../parser_workdir/step_logs'\n",
    "\n",
    "# df = get_results_opensesame(all_exps, \n",
    "#                  output_model_dir,\n",
    "#                  task_name=\"targetid\")\n",
    "\n",
    "# df = get_results_opensesame(all_exps, \n",
    "#                  output_model_dir,\n",
    "#                  task_name=\"frameid\")\n",
    "\n",
    "df = get_results_opensesame(all_exps, \n",
    "                 output_model_dir,\n",
    "                 task_name=\"argid\")\n",
    "\n",
    "df = prettify(df)\n",
    "\n",
    "\n",
    "\n",
    "columns = ['preds_model', 'pipeline', 'dataset', 'task', 'f1', 'pre', 'rec']\n",
    "columns = ['preds_model', 'pipeline', 'dataset', 'task', 'f1']\n",
    "# df = df[columns]\n",
    "df\n",
    "\n",
    "# check number of runs per dataset/preds_model\n",
    "\n",
    "# group_columns = ['preds_model', 'pipeline', 'dataset', 'task']\n",
    "\n",
    "# dfg=df.groupby(group_columns, as_index=False).agg(['count']).reset_index()\n",
    "\n",
    "# dfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_statistical_measures(df)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-moldova",
   "metadata": {},
   "source": [
    "## Visualize - Learning Curve over  F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-gazette",
   "metadata": {},
   "source": [
    "### List of paths to experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_exps_dir = \"nPc_verbs_randAllExps\"\n",
    "seed_exps = \"\"\"010pc_verbs\n",
    "020pc_verbs\n",
    "030pc_verbs\n",
    "040pc_verbs\n",
    "050pc_verbs\n",
    "100pc_verbs\"\"\".split('\\n')\n",
    "seed_exps = [f\"{seed_exps_dir}/{exp}\" for exp in seed_exps]\n",
    "\n",
    "\n",
    "expanded_exps_dir = 'expanded_nPc_verbs_randAllExps'\n",
    "\n",
    "expanded_exps1 = \"\"\"010pc_verbs_expanded_nouns-50pc\n",
    "020pc_verbs_expanded_nouns-50pc\n",
    "030pc_verbs_expanded_nouns-50pc\n",
    "040pc_verbs_expanded_nouns-50pc\n",
    "050pc_verbs_expanded_nouns-50pc\n",
    "100pc_verbs_expanded_nouns-50pc\"\"\".split('\\n')\n",
    "\n",
    "expanded_exps2 = \"\"\"010pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "020pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "030pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "040pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "050pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "100pc_verbs_expanded_lu_roles_nouns-50pc\"\"\".split('\\n')\n",
    "\n",
    "pipeline1 = 'lugold_rolegold_nltk_nolemma_N2'\n",
    "pipeline2 = 'lugold_rolegold_nltk_nolemma_role_stopwords_N2'\n",
    "\n",
    "preds_model = 'xlnet_embs_hypers'\n",
    "exps11 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps12 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps11.extend(exps12)\n",
    "\n",
    "preds_model = 'bert'\n",
    "exps21 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps22 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps21.extend(exps22)\n",
    "\n",
    "exps = seed_exps\n",
    "exps.extend(exps11)\n",
    "exps.extend(exps21)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "all_exps = exps\n",
    "\n",
    "print(len(all_exps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = '../parser_workdir/step_logs'\n",
    "df = get_results_opensesame(all_exps, output_model_dir)\n",
    "\n",
    "\n",
    "\n",
    "df = prettify2(df)\n",
    "# df\n",
    "# columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "# group_columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "columns = ['preds_model', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "group_columns = ['preds_model', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "df = df[columns]\n",
    "df\n",
    "# check number of runs per dataset/preds_model\n",
    "# dfg=df.groupby(group_columns, as_index=False).agg(['count']).reset_index()\n",
    "\n",
    "# dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-sessions",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df = compute_statistical_measures_lr(df)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "def results_maps(dfg, \n",
    "                 base_dataset = 'nPercentData',\n",
    "                 base_model=\"base\",\n",
    "                 augmented_datasets = ['augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc'],\n",
    "                 augmented_models = ['bert', 'xlnet_embs_hypers'],\n",
    "                 eval_measure=\"f1\"):\n",
    "\n",
    "    mean_scores = {}\n",
    "    std_scores = {}\n",
    "    p_values = {}\n",
    "#     base model scores\n",
    "    scores = dfg.loc[(dfg['dataset'] == base_dataset) & (dfg['preds_model'] == base_model)][eval_measure]['mean'].tolist()\n",
    "    mean_scores[base_model] = {base_dataset:scores}\n",
    "\n",
    "    scores = dfg.loc[(dfg['dataset'] == base_dataset) & (dfg['preds_model'] == base_model)][eval_measure]['std'].tolist()\n",
    "    std_scores[base_model] = {base_dataset:scores}\n",
    "\n",
    "# augmented model scores\n",
    "    for m in augmented_models:\n",
    "\n",
    "        all_models_means = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)][eval_measure]['mean'].tolist()\n",
    "            all_models_means[ds] = scores\n",
    "\n",
    "        all_models_std = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)][eval_measure]['std'].tolist()\n",
    "            all_models_std[ds] = scores\n",
    "\n",
    "        all_models_ps = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)]['p_value'].tolist()\n",
    "            all_models_ps[ds] = scores\n",
    "            \n",
    "        mean_scores[m] = all_models_means\n",
    "        std_scores[m] = all_models_std\n",
    "        p_values[m] = all_models_ps\n",
    "        \n",
    "    return mean_scores, std_scores, p_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores, std_scores, p_values = results_maps(results_df)\n",
    "mean_scores, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "%matplotlib inline\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "N = [10,20,30,40,50, 100]\n",
    "\n",
    "base_model = 'base'\n",
    "augmented_models = ['bert', 'xlnet_embs_hypers']\n",
    "\n",
    "datasets = ['nPercentData', 'augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc']\n",
    "labels = ['seed', 'augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc']\n",
    "\n",
    "title = \"Learning Curves\"\n",
    "subtitles = {'bert': 'BERT',\n",
    "            'xlnet_embs_hypers': 'XLNet+embs'}\n",
    "\n",
    "colors = ['k','c', 'y', 'r', 'brown']\n",
    "# ----------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, \n",
    "                         dpi=480,\n",
    "                         figsize=(14, 4))\n",
    "\n",
    "for j, m in enumerate(augmented_models):\n",
    "    print(j, m)\n",
    "\n",
    "    axes[j].set_title(f'Substitution model: {subtitles[m]}')\n",
    "    axes[j].set_xlabel('Percentage of training data sampled randomly (dataset: Verbs)')\n",
    "    axes[0].set_ylabel('F1-score (with gold frames)')\n",
    "    axes[j].set_xticks(N)\n",
    "\n",
    "    axes[j].set_yticks(range(26, 62, 2))\n",
    "    axes[j].grid()\n",
    "         \n",
    "    i = -1\n",
    "    for ds, label in zip(datasets, labels):\n",
    "        i += 1\n",
    "        print(ds, label)\n",
    "        if label == 'seed': model = base_model\n",
    "        else:\n",
    "            model = m   \n",
    "            ps = p_values[model][ds]\n",
    "            # ------------------------------- to place P_values on optimally\n",
    "            exp_names = list(mean_scores[model].keys())\n",
    "            max_mean = [max(a,b) for a,b in zip(np.array(mean_scores[model][exp_names[0]]), np.array(mean_scores[model][exp_names[-1]]))]\n",
    "            max_std = [max(a,b) for a,b in zip(np.array(std_scores[model][exp_names[0]]), np.array(std_scores[model][exp_names[-1]]))]\n",
    "#             pY = [x+y for x,y in zip(max_mean, max_std)]\n",
    "            pY = max_mean\n",
    "        #     -------------------------------\n",
    "            \n",
    "\n",
    "        mean = np.array(mean_scores[model][ds])\n",
    "        std = np.array(std_scores[model][ds])\n",
    "        \n",
    "        axes[j].fill_between(N, mean - std,\n",
    "                             mean + std, alpha=0.1,\n",
    "                             color=colors[i])\n",
    "\n",
    "        axes[j].semilogx(N, mean, 'o-', color=colors[i],\n",
    "                     label=labels[i], base=2, nonpositive='clip')\n",
    "        pX = N\n",
    "        p = -1\n",
    "        if i==1:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 7\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "                #   place it below the lines\n",
    "                if xx in [40, 100]:\n",
    "                    axes[j].text(xx,yy-5, ps[p], fontdict=font)        \n",
    "                #  place it above the lines\n",
    "                else:\n",
    "                    axes[j].text(xx,yy+3, ps[p], fontdict=font) \n",
    "        if i==2:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 7\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "                #   place it below the lines\n",
    "                if xx in [40, 100]:\n",
    "                    axes[j].text(xx,yy-6.5, ps[p], fontdict=font)        \n",
    "                #  place it above the lines\n",
    "                else:\n",
    "                    axes[j].text(xx,yy+4.5, ps[p], fontdict=font) \n",
    "   \n",
    "\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes[:1]]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "plt.figlegend( lines, labels, loc = 'lower center', ncol=3, labelspacing=0. , bbox_to_anchor=(0.5, -0.1))\n",
    "# axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-music",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
