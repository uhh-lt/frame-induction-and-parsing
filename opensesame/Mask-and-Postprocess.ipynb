{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataframe  to predict/postprocess predictions only once\n",
    "\n",
    "- do it for both verbs and nouns separately\n",
    "with all possible options, merge the dataframes and then use it to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sesame.dataio import read_conll\n",
    "from sesame.conll09 import CoNLL09Example, CoNLL09Element\n",
    "from sesame.sentence import Sentence\n",
    "import sys\n",
    "import lexsub.conll_helper as conll_helper\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "data_dir = '../parser_workdir/data/open_sesame_v1_data/fn1.7'\n",
    "\n",
    "exp = 'verbs'\n",
    "\n",
    "dev_file = 'fn1.7.dev.syntaxnet.conll'\n",
    "test_file = 'fn1.7.test.syntaxnet.conll'\n",
    "train_file = 'fn1.7.fulltext.train.syntaxnet.conll'\n",
    "\n",
    "\n",
    "input_file = f'{data_dir}/{exp}/{train_file}'\n",
    "\n",
    "examples, __, __ = read_conll(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lexsub import augment_conll\n",
    "from lexsub.augment_conll import PROC_FUNCS_OPTIONS, mask_sentences\n",
    "\n",
    "parser = 'nltk' # for goldcluster\n",
    "add_goldclusters = False\n",
    "df1, config_dicts = mask_sentences(examples, \n",
    "                                   substitute_lu=True,\n",
    "                                   substitute_role=True, role_tokens=[1], #role_postags='NN,NNS,NNP,NNPS',\n",
    "                                   noun_max=1, ibn=True,\n",
    "                                   parser=parser,\n",
    "                                   add_goldclusters=add_goldclusters,\n",
    "                                           verbose=False)\n",
    "\n",
    "len(examples), len(config_dicts), len(df1)\n",
    "# df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, config_dicts2 = mask_sentences(examples, \n",
    "                                    substitute_lu=False,\n",
    "                                    substitute_role=False, role_tokens=[1], #role_postags='NN,NNS,NNP,NNPS',\n",
    "                                    noun_max=1, ibn=True,\n",
    "                                    parser=parser,\n",
    "                                    add_goldclusters=add_goldclusters,\n",
    "                                           verbose=False)\n",
    "\n",
    "len(examples), len(config_dicts2), len(df2)\n",
    "# df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.append(df2)\n",
    "df['index'] = df['index'].apply(lambda x: x[0])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.drop_duplicates(subset=['sentence', 'index']).reset_index(drop=True)\n",
    "print(len(dff))\n",
    "\n",
    "dff.to_pickle('../workdir/data/swv_fsp_T.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To postprocess predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.drop_duplicates(subset=['sentence', 'index']).reset_index(drop=True)\n",
    "df_pp = df.drop_duplicates(subset=['sentence', 'index', 'word_type']).reset_index(drop=True)\n",
    "# for postprocessing you also need word_type\n",
    "print(len(dff)), print(len(df_pp))\n",
    "dff.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read model predictions [only for post-processing once]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "results = '../workdir/results_parser/verb_preds_st'\n",
    "df = pd.read_pickle('../workdir/data/swv_fsp_T.pkl').reset_index(drop=True)\n",
    "\n",
    "# results = '../workdir/results_parser/noun_preds_st'\n",
    "# df = pd.read_pickle('../workdir/data/swn_fsp_T.pkl').reset_index(drop=True)\n",
    "\n",
    "# df['index'] = df['index'].apply(lambda x: x[0])\n",
    "print(len(df))\n",
    "\n",
    "subst_dir=results\n",
    "model2res = {}\n",
    "for p in Path(subst_dir).glob('*/results.csv'):\n",
    "        print(p.parent.name)\n",
    "        model2res[p.parent.name] = p\n",
    "        \n",
    "        \n",
    "\n",
    "for p in Path(subst_dir).glob('*/predictions.pkl'):\n",
    "#     print(p)\n",
    "    print(p.parent.name)\n",
    "    model2res[p.parent.name] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "# 'gie_sss_test_semiPURExlnet_embs', \n",
    "'gie_sss_test_semiPURExlnet_embs_swrhypers', \n",
    "# 'gie_sss_test_semiPURExlnet_embs_swnhypers', \n",
    "'gie_sss_test_semiPURExlnet_embs_swvhypers',\n",
    "'blc-ntok1-nunits1-nomask-k200',\n",
    "# 'bbc-ntok1-nunits1-nomask-k200',\n",
    "# 'fasttext_cc_nolem',\n",
    "# 'glove_840B_nolem',\n",
    "# 'dt_wiki_lem',\n",
    "# 'dt_59g_lem',\n",
    "# 'melamud_baladd'\n",
    "]\n",
    "\n",
    "preds_dfs = {}\n",
    "\n",
    "for k,v in model2res.items() :\n",
    "    if not k in models: continue\n",
    "    print(k)\n",
    "    if str(v).endswith('.pkl'):\n",
    "        df1 = df.copy()\n",
    "        df1['pred_substitutes'] = pd.read_pickle(v)\n",
    "        sdf = df1.copy()\n",
    "    else:\n",
    "        if not 'semiPURExlnet' in str(v): continue\n",
    "        sdf = pd.read_csv(v, \n",
    "                  converters={'pred_substitutes':eval, 'context':eval})\n",
    "    \n",
    "        sdf['sentence'] = sdf.context.str.join('')\n",
    "        sdf['st'] = sdf.apply(lambda r: len(''.join(r.context[:r.target_position])), axis=1)\n",
    "        sdf['index'] = sdf.apply(lambda r: (r.st, r.st+len(r.context[r.target_position].strip())), axis=1)\n",
    "        \n",
    "    preds_dfs[k] = sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 copy all predictions to one dataframe, if needs to post process, then use use dataframe from step#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pp.copy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get for each input example its substitutes by merging dataframes on (index, sentence) key\n",
    "def add_substs(idf, sdf):\n",
    "    print(len(idf), len(sdf))\n",
    "    mdf = idf.merge(sdf[['index','sentence','pred_substitutes']], how='left', on=['index','sentence'], \n",
    "                       validate='many_to_one')\n",
    "    assert mdf.pred_substitutes.isnull().sum()==0 # check that all input examples received substitutes!\n",
    "    return mdf\n",
    "\n",
    "\n",
    "models = []\n",
    "for k,v in preds_dfs.items():\n",
    "    print(k)\n",
    "    mdf = add_substs(df.copy(), v)\n",
    "    df[k.replace('gie_sss_test_', '')] = mdf['pred_substitutes']\n",
    "    models.append(k.replace('gie_sss_test_', ''))\n",
    "#     break\n",
    "# df['xlnet_embs_hypers'] = df.apply(lambda row: row['semiPURExlnet_embs_swnhypers'] if row['word_type']=='lu_n' else row['semiPURExlnet_embs_swrhypers'], axis=1)\n",
    "df['xlnet_embs_hypers'] = df.apply(lambda row: row['semiPURExlnet_embs_swvhypers'] if row['word_type']=='lu_v' else row['semiPURExlnet_embs_swrhypers'], axis=1)\n",
    "df = df.rename(columns={'blc-ntok1-nunits1-nomask-k200':'bert'})\n",
    "\n",
    "# df['bert_best'] = df.apply(lambda row: row['blc-ntok1-nunits1-nomask-k200'] if row['word_type']=='lu_v' else row['bbc-ntok1-nunits1-mask-k200'], axis=1)\n",
    "# df['bert_best'] = df.apply(lambda row: row['bbc-ntok1-nunits1-nomask-k200'] if row['word_type']=='lu_n' else row['bbc-ntok1-nunits1-mask-k200'], axis=1)\n",
    "\n",
    "# df['dt'] = df.apply(lambda row: row['dt_59g_lem'] if row['word_type']=='lu' else row['dt_wiki_lem'], axis=1)\n",
    "# df['embs'] = df.apply(lambda row: row['fasttext_cc_nolem'] if row['word_type']=='lu' else row['glove_840B_nolem'], axis=1)\n",
    "\n",
    "models.append('xlnet_embs_hypers')\n",
    "models.append('bert')\n",
    "# models.append('dt')\n",
    "# models.append('embs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['sentence', 'masked_sent','index', 'seed_word', 'gold_cluster', 'word_type', 'postag', 'identifier', 'e_id', 'ex_id',\n",
    "#           'bert', 'xlnet_embs_hypers', 'dt', 'embs','melamud_baladd']\n",
    "columns = ['sentence', 'masked_sent','index', 'seed_word', 'gold_cluster', 'word_type', 'postag', 'identifier', 'e_id', 'ex_id',\n",
    "          'bert', 'xlnet_embs_hypers']\n",
    "df_post = df[columns].copy()\n",
    "df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Post-process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    " 'bert',\n",
    " 'xlnet_embs_hypers',\n",
    "#  'dt',\n",
    "#  'embs',\n",
    "#  'melamud_baladd',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_FUNCS_OPTIONS={\n",
    "'nolemma' : {\n",
    "            'lu_v': 'clean_noisy,filter_vocab,match_inflection',\n",
    "            'lu_n': 'clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns,match_inflection' ,\n",
    "            'role' : 'clean_noisy',\n",
    "            'noun':'clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns,match_inflection'\n",
    "            },\n",
    "    \n",
    "'lemma' : {\n",
    "            'lu_v': 'lemmatize,clean_noisy,filter_vocab',\n",
    "            'lu_n': 'lemmatize,clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns',\n",
    "            'role' : 'lemmatize,clean_noisy',\n",
    "            'noun':'lemmatize,clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns'\n",
    "            },\n",
    "\n",
    "'nolemma_role_stopwords' : {\n",
    "                    'lu_v': 'clean_noisy,filter_vocab,match_inflection',\n",
    "                    'lu_n': 'clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns,match_inflection',\n",
    "                    'role' : 'clean_noisy,remove_role_stopwords',\n",
    "                    'noun':'clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns,match_inflection'\n",
    "                    },\n",
    "'lemma_role_stopwords' : {\n",
    "                    'lu_v': 'lemmatize,clean_noisy, filter_vocab',\n",
    "                    'lu_n': 'lemmatize,clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns',\n",
    "                    'role' : 'lemmatize,clean_noisy,remove_role_stopwords',\n",
    "                    'noun':'lemmatize,clean_noisy,remove_digits,remove_noun_stopwords,filter_nouns'\n",
    "                    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df_post.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lexsub.augment_conll import postprocess_predictions\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "    df_post[f'{m}_final_preds'] = postprocess_predictions(temp_df, df_post[f'{m}'].tolist(),\n",
    "                                                    proc_funcs=PROC_FUNCS_OPTIONS['nolemma'],\n",
    "                                                         verbose=True)\n",
    "    \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.to_pickle(\"../workdir/data/swv_nltk_nolemma_final_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lexsub.augment_conll import postprocess_predictions\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "    df_post[f'{m}_final_preds'] = postprocess_predictions(temp_df, df_post[f'{m}'].tolist(),\n",
    "                                                    proc_funcs=PROC_FUNCS_OPTIONS['nolemma_role_stopwords'],\n",
    "                                                         verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.to_pickle(\"../workdir/data/swv_nltk_nolemma_role_stopwords_final_predictions.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.loc[df_post['word_type']=='role']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Copy predictions to each expanded dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    " 'bert',\n",
    " 'xlnet_embs_hypers',\n",
    "#  'dt',\n",
    "#  'embs',\n",
    "#  'melamud_baladd',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_prefix = 'swv'\n",
    "parser = 'nltk'\n",
    "# pipeline = 'nolemma'\n",
    "pipeline = 'nolemma_role_stopwords'\n",
    "\n",
    "predictions_file = f'{data_prefix}_{parser}_{pipeline}_final_predictions.pkl'\n",
    "\n",
    "df_post = pd.read_pickle(f'../workdir/data/{predictions_file}')\n",
    "\n",
    "src_df = df_post.copy()\n",
    "# just need predictions, regardless of word_type\n",
    "src_df = src_df.drop_duplicates(subset=['sentence', 'index']).reset_index(drop=True)\n",
    "\n",
    "src_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "# Get for each input example its substitutes by merging dataframes on (index, sentence) key\n",
    "def add_substs(idf, sdf, preds_column):\n",
    "#     print(len(idf)), print(len(sdf))\n",
    "    mdf = idf.merge(sdf, how='left', on=['index','sentence'], \n",
    "                       validate='many_to_one')\n",
    "    assert mdf[preds_column].isnull().sum()==0 # check that all input examples received substitutes!\n",
    "    return mdf\n",
    "\n",
    "# exps_dir = 'expanded_nPc_verbs_randAllExps'\n",
    "exps_dir = 'expanded_nExPerSent_verbs_randAllExps'\n",
    "# exps_dir = 'expanded_nPc_nouns_randAllExps'\n",
    "# exps_dir = 'expanded_nExPerSent_nouns_randAllExps'\n",
    "exps = '*'\n",
    "for f in glob(f'../parser_workdir/data/open_sesame_v1_data/fn1.7/{exps_dir}/{exps}/data.pkl'):\n",
    "    if not 'roles' in f: continue\n",
    "    print(f)\n",
    "    path = f.replace('/data.pkl', '')\n",
    "    idf= pd.read_pickle(f)\n",
    "    idf['index']=idf['index'].str[0]  # need hashable tuple instead of non-hashable list\n",
    "    for m in models:\n",
    "        print(m)\n",
    "        preds_column = m\n",
    "        sdf = src_df[['index','sentence', preds_column]].copy()\n",
    "        mdf = add_substs(idf.copy(), sdf, preds_column)\n",
    "        predictions = mdf[preds_column].tolist() \n",
    "#         break\n",
    "#     break     \n",
    "\n",
    "        dir_path = f'{path}/{m}'\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path) \n",
    "        \n",
    "        with open(f\"{dir_path}/predictions.pkl\", 'wb') as fp:\n",
    "            pickle.dump(predictions, fp)\n",
    "            \n",
    "            \n",
    "#     break # remote this break to load all input dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Copy final_predictions to each expanded dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_prefix = 'swv'\n",
    "parser = 'nltk'\n",
    "# pipeline = 'nolemma'\n",
    "pipeline = 'nolemma_role_stopwords'\n",
    "\n",
    "predictions_file = f'{data_prefix}_{parser}_{pipeline}_final_predictions.pkl'\n",
    "print(predictions_file)\n",
    "df_post = pd.read_pickle(f'../workdir/data/{predictions_file}')\n",
    "\n",
    "src_df = df_post.copy()\n",
    "column_postfix = \"final_preds\"\n",
    "src_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get for each input example its substitutes by merging dataframes on (index, sentence) key\n",
    "def add_substs(idf, sdf, preds_column):\n",
    "    mdf = idf.merge(sdf, how='left', on=['word_type', 'index','sentence'], \n",
    "                       validate='many_to_one')\n",
    "    assert mdf[preds_column].isnull().sum()==0 # check that all input examples received substitutes!\n",
    "    return mdf\n",
    "\n",
    "\n",
    "# exps_dir = 'expanded_nPc_verbs_randAllExps'\n",
    "exps_dir = 'expanded_nExPerSent_verbs_randAllExps'\n",
    "# exps_dir = 'expanded_nPc_nouns_randAllExps'\n",
    "# exps_dir = 'expanded_nExPerSent_nouns_randAllExps'\n",
    "exps = '*'\n",
    "\n",
    "for f in glob(f'../parser_workdir/data/open_sesame_v1_data/fn1.7/{exps_dir}/{exps}/data.pkl'):\n",
    "#     if not 'roles' in f: continue\n",
    "    print(f.replace('../parser_workdir/data/open_sesame_v1_data/fn1.7/', ''))\n",
    "    path = f.replace('/data.pkl', '')\n",
    "    idf=pd.read_pickle(f)\n",
    "    idf['index']=idf['index'].str[0]  # need hashable tuple instead of non-hashable list\n",
    "    for m in models:\n",
    "        print(m)\n",
    "        preds_column = f'{m}_{column_postfix}'\n",
    "        sdf = src_df[['word_type', 'index','sentence', preds_column]].copy()\n",
    "        mdf = add_substs(idf.copy(), sdf, preds_column)\n",
    "        predictions = mdf[preds_column].tolist() \n",
    "        #break    \n",
    "        dir_path = f'{path}/{m}'\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path) \n",
    "        \n",
    "        with open(f\"{dir_path}/{parser}_{pipeline}_final_predictions.pkl\", 'wb') as fp:\n",
    "            pickle.dump(predictions, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
