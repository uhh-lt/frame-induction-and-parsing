{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-turner",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = '../parser_workdir'\n",
    "OUTPUT_DIR = f'{PATH}/srl_parser_goldpredicate'\n",
    "OUTPUT_DIR = f'{PATH}/srl_parser'\n",
    "runs = '01,02,03,04,05,06,07,08,09,10'.split(',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-liver",
   "metadata": {},
   "source": [
    "### Seed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-harvest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "exps_dir = 'nPc_verbs_randAllExps'\n",
    "exps = [\n",
    "'010pc_verbs',\n",
    "'020pc_verbs',\n",
    "'030pc_verbs',\n",
    "'040pc_verbs',\n",
    "'050pc_verbs',\n",
    "'100pc_verbs',\n",
    "]\n",
    "\n",
    "for exp in exps:\n",
    "    print(exp)\n",
    "    EXP = f'{exps_dir}/{exp}'\n",
    "#     break\n",
    "    for run in runs:\n",
    "        rescode=os.system(f'HYDRA_CONFIG_PATH=../configurations/run_evaluate_srlparser.yaml python -m sesame.run_evaluate output_dir={OUTPUT_DIR} exp_dir={EXP} model_run={run}')\n",
    "        print(f'run:{run}-->{rescode}')\n",
    "        if rescode!=0:\n",
    "            break\n",
    "    if rescode!=0:\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-fields",
   "metadata": {},
   "source": [
    "### Augmented Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-lemon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "exps_dir = 'expanded_nPc_verbs_randAllExps'\n",
    "exps = [\n",
    "'010pc_verbs_expanded_nouns-50pc',\n",
    "'020pc_verbs_expanded_nouns-50pc',\n",
    "'030pc_verbs_expanded_nouns-50pc',\n",
    "'040pc_verbs_expanded_nouns-50pc',\n",
    "'050pc_verbs_expanded_nouns-50pc',\n",
    "'100pc_verbs_expanded_nouns-50pc',\n",
    "'010pc_verbs_expanded_lu_roles_nouns-50pc',\n",
    "'020pc_verbs_expanded_lu_roles_nouns-50pc',\n",
    "'030pc_verbs_expanded_lu_roles_nouns-50pc',\n",
    "'040pc_verbs_expanded_lu_roles_nouns-50pc',\n",
    "'050pc_verbs_expanded_lu_roles_nouns-50pc',\n",
    "'100pc_verbs_expanded_lu_roles_nouns-50pc'\n",
    "]\n",
    "\n",
    "pipeline = \"lugold_rolegold_nltk_nolemma_role_stopwords_N2\"\n",
    "pipeline = \"lugold_rolegold_nltk_nolemma_N2\"\n",
    "\n",
    "\n",
    "for exp in exps:\n",
    "    print(exp)\n",
    "    preds_model = 'xlnet_embs_hypers'\n",
    "    EXP = f'{exps_dir}/{exp}/{preds_model}/{pipeline}'\n",
    "    for run in runs:\n",
    "        rescode=os.system(f'HYDRA_CONFIG_PATH=../configurations/run_evaluate_srlparser.yaml python -m sesame.run_evaluate output_dir={OUTPUT_DIR} exp_dir={EXP} model_run={run}')\n",
    "        print(f'run:{run}-->{rescode}')\n",
    "        \n",
    "for exp in exps:\n",
    "    preds_model = 'bert'\n",
    "    EXP = f'{exps_dir}/{exp}/{preds_model}/{pipeline}'\n",
    "    print(exp)\n",
    "    for run in runs:\n",
    "        rescode=os.system(f'HYDRA_CONFIG_PATH=../configurations/run_evaluate_srlparser.yaml python -m sesame.run_evaluate output_dir={OUTPUT_DIR} exp_dir={EXP} model_run={run}')\n",
    "        print(f'run:{run}-->{rescode}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-intellectual",
   "metadata": {},
   "source": [
    "# Visualize - Learning Curve over  F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub.results_parser import parse_results, ttest_df, ttest_df\n",
    "from lexsub.results_parser import compute_statistical_measures,  compute_statistical_measures_lr\n",
    "from lexsub.results_parser import get_results_bertSRL, prettify, prettify2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prettify2(df):\n",
    "\n",
    "#     df['preds_model'] = df['exp_path'].apply(lambda x: x.split('/')[2] if 'expanded' in x else 'base')\n",
    "#     df['dataset'] = df['exp_path'].apply(lambda x: x.split('/')[1])\n",
    "#     df['pipeline'] = df['exp_path'].apply(lambda x: x.split('/')[3] if 'expanded' in x else '')\n",
    "#     df['sample_size'] = df['dataset'].apply(lambda x: int(x[:3]))\n",
    "\n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: x.split('pc_')[-1])\n",
    "    \n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: x.replace('verbs_expanded_','augmented_'))\n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: x.replace('nouns_expanded_','augmented_'))\n",
    "\n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: \"nPercentData\" if x== 'verbs' or x == 'nouns' else x)\n",
    "\n",
    "\n",
    "    \n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: x.replace('_', '-'))\n",
    "#     df['dataset'] = df['dataset'].apply(lambda x: x.replace('lu', 'lexical unit'))\n",
    "    \n",
    "#     df = df.round(2)\n",
    "    \n",
    "#     return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-logic",
   "metadata": {},
   "source": [
    "### List of paths to experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_exps_dir = \"nPc_verbs_randAllExps\"\n",
    "\n",
    "seed_exps = \"\"\"010pc_verbs\n",
    "020pc_verbs\n",
    "030pc_verbs\n",
    "040pc_verbs\n",
    "050pc_verbs\n",
    "100pc_verbs\"\"\".split('\\n')\n",
    "seed_exps = [f\"{seed_exps_dir}/{exp}\" for exp in seed_exps]\n",
    "\n",
    "\n",
    "expanded_exps_dir = 'expanded_nPc_verbs_randAllExps'\n",
    "\n",
    "expanded_exps1 = \"\"\"010pc_verbs_expanded_nouns-50pc\n",
    "020pc_verbs_expanded_nouns-50pc\n",
    "030pc_verbs_expanded_nouns-50pc\n",
    "040pc_verbs_expanded_nouns-50pc\n",
    "050pc_verbs_expanded_nouns-50pc\n",
    "100pc_verbs_expanded_nouns-50pc\"\"\".split('\\n')\n",
    "\n",
    "expanded_exps2 = \"\"\"010pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "020pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "030pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "040pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "050pc_verbs_expanded_lu_roles_nouns-50pc\n",
    "100pc_verbs_expanded_lu_roles_nouns-50pc\"\"\".split('\\n')\n",
    "\n",
    "pipeline1 = 'lugold_rolegold_nltk_nolemma_N2'\n",
    "pipeline2 = 'lugold_rolegold_nltk_nolemma_role_stopwords_N2'\n",
    "\n",
    "preds_model = 'xlnet_embs_hypers'\n",
    "exps11 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps12 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps11.extend(exps12)\n",
    "\n",
    "preds_model = 'bert'\n",
    "exps21 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline1}' for exp in expanded_exps1]\n",
    "exps22 = [f'{expanded_exps_dir}/{exp}/{preds_model}/{pipeline2}' for exp in expanded_exps2]\n",
    "\n",
    "exps21.extend(exps22)\n",
    "\n",
    "exps = seed_exps\n",
    "exps.extend(exps11)\n",
    "exps.extend(exps21)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "all_exps = exps\n",
    "\n",
    "print(len(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-thriller",
   "metadata": {},
   "source": [
    "## Aggregate results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## calculate mean, std, p-value for Experiment-2: learning curve\n",
    "# def compute_statistical_measures_lr(df,\n",
    "#                                   eval_measure=\"f1\",\n",
    "#                                   verbose=False):\n",
    "    \n",
    "#     group_columns = ['preds_model', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "#     dfg=df.groupby(group_columns, as_index=False).agg(['mean', 'std']).reset_index()\n",
    "#     dfg = dfg.round(2)\n",
    "#     ps = []\n",
    "\n",
    "#     for m, ds, n in zip(dfg['preds_model'], dfg['dataset'], dfg['sample_size']):\n",
    "\n",
    "#         sample1 = df.loc[(df['preds_model']=='base') & (df['sample_size']==n)][eval_measure]\n",
    "#         sample2 = df.loc[(df['preds_model']==m) & (df['dataset']==ds) & (df['sample_size']==n)][eval_measure]\n",
    "#         if verbose:\n",
    "#             if m!=\"base\":\n",
    "#                 print(n, m, ds)\n",
    "#                 print(\"sample1:\", list(sample1))\n",
    "#                 print(\"sample2:\", list(sample2))\n",
    "#                 print(ttest(sample1, sample2))\n",
    "#         ps.append(ttest(sample1, sample2))\n",
    "\n",
    "#     dfg['p_value'] = ps\n",
    "#     dfg\n",
    "#     return dfg\n",
    "\n",
    "\n",
    "# # seed_df = df.loc[df['preds_model']=='base'].copy()\n",
    "# # # dfg['p_value'] = dfg.apply(lambda row: ttest(seed_df.loc[seed_df['task']==row['task'].iloc[0]]['f1'], \n",
    "# # #                                              df.loc[(df['preds_model']==row['preds_model'].iloc[0])  & (df['task']==row['task'].iloc[0]) & (df['exp']==row['exp'].iloc[0])]['f1']), \n",
    "# # #                            axis=1)\n",
    "# # dfg['p_value'] = dfg.apply(lambda row: ttest_df(row, seed_df, df, verbose=False),\n",
    "# #                                 axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_bertSRL(exps, output_model_dir, verbose=False):\n",
    "    df = pd.DataFrame(columns=['exp_path', 'task', 'run#', 'f1', 'pre', 'rec'])\n",
    "    for exp in exps:\n",
    "        print(exp)\n",
    "        for model in glob(f'{output_model_dir}/{exp}/*'):\n",
    "            print(model)\n",
    "            try:\n",
    "                run_num = int(model.split('/')[-1].split('-')[-1])\n",
    "            except:continue    \n",
    "            try:\n",
    "                f1, p, r = parse_results(f'{model}/test-f1.txt')\n",
    "\n",
    "                task_name = 'argid' \n",
    "                df.loc[len(df)] = [exp, task_name, run_num, f1, p, r]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-deployment",
   "metadata": {},
   "source": [
    "### Without gold frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-grade",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH = '../parser_workdir'\n",
    "output_model_dir = f'{PATH}/srl_parser'\n",
    "\n",
    "df = get_results_bertSRL(all_exps, output_model_dir)\n",
    "df = prettify2(df)\n",
    "# df\n",
    "# columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "# group_columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "columns = ['preds_model', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "group_columns = ['preds_model', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "df1 = df[columns]\n",
    "\n",
    "# check number of runs per dataset/preds_model\n",
    "dfg=df1.groupby(group_columns, as_index=False).agg(['count']).reset_index()\n",
    "\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df1 = compute_statistical_measures_lr(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-techno",
   "metadata": {},
   "source": [
    "### With gold frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = f'{PATH}/srl_parser_goldpredicate'\n",
    "df = get_results_bertSRL(all_exps, output_model_dir)\n",
    "df = prettify2(df)\n",
    "# df\n",
    "# columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "# group_columns = ['preds_model', 'pipeline', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "columns = ['preds_model', 'dataset', 'task', 'sample_size', 'f1', 'pre', 'rec']\n",
    "group_columns = ['preds_model', 'dataset', 'task', 'sample_size']\n",
    "\n",
    "\n",
    "df2 = df[columns]\n",
    "\n",
    "# check number of runs per dataset/preds_model\n",
    "dfg=df2.groupby(group_columns, as_index=False).agg(['count']).reset_index()\n",
    "dfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-overhead",
   "metadata": {},
   "source": [
    "## Get data in dictionaries for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "def results_maps(dfg, \n",
    "                 base_dataset = 'nPercentData',\n",
    "                 base_model=\"base\",\n",
    "                 augmented_datasets = ['augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc'],\n",
    "                 augmented_models = ['bert', 'xlnet_embs_hypers'],\n",
    "                 eval_measure=\"f1\"):\n",
    "\n",
    "    mean_scores = {}\n",
    "    std_scores = {}\n",
    "    p_values = {}\n",
    "#     base model scores\n",
    "    scores = dfg.loc[(dfg['dataset'] == base_dataset) & (dfg['preds_model'] == base_model)][eval_measure]['mean'].tolist()\n",
    "    mean_scores[base_model] = {base_dataset:scores}\n",
    "\n",
    "    scores = dfg.loc[(dfg['dataset'] == base_dataset) & (dfg['preds_model'] == base_model)][eval_measure]['std'].tolist()\n",
    "    std_scores[base_model] = {base_dataset:scores}\n",
    "\n",
    "# augmented model scores\n",
    "    for m in augmented_models:\n",
    "\n",
    "        all_models_means = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)][eval_measure]['mean'].tolist()\n",
    "            all_models_means[ds] = scores\n",
    "\n",
    "        all_models_std = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)][eval_measure]['std'].tolist()\n",
    "            all_models_std[ds] = scores\n",
    "\n",
    "        all_models_ps = {}\n",
    "        for ds in augmented_datasets:\n",
    "            scores = dfg.loc[(dfg['dataset'] == ds) & (dfg['preds_model'] == m)]['p_value'].tolist()\n",
    "            all_models_ps[ds] = scores\n",
    "            \n",
    "        mean_scores[m] = all_models_means\n",
    "        std_scores[m] = all_models_std\n",
    "        p_values[m] = all_models_ps\n",
    "        \n",
    "    return mean_scores, std_scores, p_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-strand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df1 = compute_statistical_measures_lr(df1)\n",
    "\n",
    "mean_scores, std_scores, p_values = results_maps(results_df1)\n",
    "mean_scores, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-secondary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with gold\n",
    "results_df2 = compute_statistical_measures_lr(df2)\n",
    "\n",
    "mean_scores_wg, std_scores_wg, p_values_wg = results_maps(results_df2)\n",
    "\n",
    "mean_scores_wg, p_values_wg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-commercial",
   "metadata": {},
   "source": [
    "### Learning curve with p-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "%matplotlib inline\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "N = [10,20,30,40,50, 100]\n",
    "eval_measure = 'f1'\n",
    "\n",
    "base_model = 'base'\n",
    "augmented_models = ['bert', 'xlnet_embs_hypers']\n",
    "\n",
    "datasets = ['nPercentData', 'augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc']\n",
    "labels = ['seed', 'augmented-nouns-50pc', 'augmented-lexical unit-roles-nouns-50pc']\n",
    "\n",
    "title = \"Learning Curves\"\n",
    "subtitles = {'bert': 'BERT',\n",
    "            'xlnet_embs_hypers': 'XLNet+embs'}\n",
    "\n",
    "title = \"Learning Curves\"\n",
    "y_label = 'F1-score'\n",
    "subtitles = {'bert': 'BERT',\n",
    "            'xlnet_embs_hypers': 'XLNet+embs'}\n",
    "\n",
    "X_label = \"Percentage of training data sampled randomly (dataset: Verbs)\"\n",
    "colors = ['k','c', 'y', 'r', 'brown']\n",
    "# ----------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(2, 2, \n",
    "                         dpi=480,\n",
    "                         figsize=(12,8))\n",
    "\n",
    "\n",
    "# -------------------------------------------------Without gold frames\n",
    "c = 0\n",
    "for j, m in enumerate(augmented_models):    \n",
    "    axes[c][j].set_title(f'Substitution model: {subtitles[m]}')\n",
    "    axes[c][j].set_xticks(N)\n",
    "    axes[c][j].set_yticks(range(10, 80, 5))\n",
    "    axes[c][j].grid()\n",
    "#     -------------------------------------------\n",
    "    i = -1\n",
    "    for ds, label in zip(datasets, labels):\n",
    "        \n",
    "        i += 1\n",
    "        if label == 'seed': model = base_model\n",
    "        else: \n",
    "            model = m   \n",
    "            ps = p_values_wg[model][ds]\n",
    "            # ------------------------------- to place P_values on optimally\n",
    "            exp_names = list(mean_scores_wg[model].keys())\n",
    "            max_mean = [max(a,b) for a,b in zip(np.array(mean_scores_wg[model][exp_names[0]]), np.array(mean_scores_wg[model][exp_names[-1]]))]\n",
    "            max_std = [max(a,b) for a,b in zip(np.array(std_scores_wg[model][exp_names[0]]), np.array(std_scores_wg[model][exp_names[-1]]))]\n",
    "            pY = [x+y for x,y in zip(max_mean, max_std)]\n",
    "        #     -------------------------------\n",
    "            \n",
    "            \n",
    "        mean = np.array(mean_scores_wg[model][ds])\n",
    "        std = np.array(std_scores_wg[model][ds])\n",
    "        axes[c][j].fill_between(N, mean - std,\n",
    "                             mean + std, alpha=0.1,\n",
    "                             color=colors[i])\n",
    "\n",
    "        axes[c][j].semilogx(N, mean, 'o-', color=colors[i],\n",
    "                 label=label, base=2, nonpositive='clip')\n",
    "        \n",
    "#         -------------------------------- add legends for p_values\n",
    "        pX = N\n",
    "        p = -1\n",
    "        if i==1:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 6\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "            #   place it below the lines\n",
    "                if xx in[30, 50, 100]:\n",
    "                    axes[c][j].text(xx-.5,yy-5, ps[p], fontdict=font) \n",
    "            #  place it above the lines\n",
    "                else:\n",
    "                    axes[c][j].text(xx-0.75,yy+2, ps[p], fontdict=font) \n",
    "        if i==2:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 6\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "            #   place it below the lines\n",
    "                if xx in[30, 50, 100]:\n",
    "                    axes[c][j].text(xx-0.5,yy-7.5, ps[p], fontdict=font)    \n",
    "            #  place it above the lines\n",
    "                else:\n",
    "                    axes[c][j].text(xx-0.75,yy+4.5, ps[p], fontdict=font) \n",
    "\n",
    "                    \n",
    "# ------------------------------------------------- With gold frames\n",
    "        \n",
    "axes[0][c].set_ylabel(f'{y_label} (with gold frames)')\n",
    "axes[-1][c].set_xlabel(X_label)\n",
    "c =1\n",
    "\n",
    "for j, m in enumerate(augmented_models):    \n",
    "    axes[c][j].set_xticks(N)\n",
    "    axes[c][j].set_yticks(range(10, 80, 5))\n",
    "    axes[c][j].grid()\n",
    "#     -------------------------------------------\n",
    "    i = -1    \n",
    "    for ds, label in zip(datasets, labels):\n",
    "        \n",
    "        i += 1\n",
    "        if label == 'seed': model = base_model\n",
    "        else: \n",
    "            model = m \n",
    "            ps = p_values[model][ds]\n",
    "            # ------------------------------- to place P_values on optimally\n",
    "            exp_names = list(mean_scores_wg[model].keys())\n",
    "            max_mean = [max(a,b) for a,b in zip(np.array(mean_scores[model][exp_names[0]]), np.array(mean_scores[model][exp_names[-1]]))]\n",
    "            max_std = [max(a,b) for a,b in zip(np.array(std_scores[model][exp_names[0]]), np.array(std_scores[model][exp_names[-1]]))]\n",
    "            pY = [x+y for x,y in zip(max_mean, max_std)]\n",
    "        #     -------------------------------    \n",
    "            \n",
    "        mean = np.array(mean_scores[model][ds])\n",
    "        std = np.array(std_scores[model][ds])\n",
    "        \n",
    "        axes[c][j].fill_between(N, mean - std,\n",
    "                             mean + std, alpha=0.1,\n",
    "                             color=colors[i])\n",
    "\n",
    "        axes[c][j].semilogx(N, mean, 'o-', color=colors[i],\n",
    "                 label=label, base=2, nonpositive='clip')\n",
    "#         -------------------------------- add legends for p_values  \n",
    "\n",
    "        pX = N\n",
    "        p = -1\n",
    "        if i==1:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 6\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "            #   place it below the lines\n",
    "                if xx in[30, 50, 100]:\n",
    "                    axes[c][j].text(xx-.5,yy-5, ps[p], fontdict=font) \n",
    "            #  place it above the lines\n",
    "                else:\n",
    "                    axes[c][j].text(xx-0.75,yy+2, ps[p], fontdict=font) \n",
    "        if i==2:\n",
    "            font = {\n",
    "#                 'family': 'serif',\n",
    "                    'color':  colors[i],\n",
    "                    'weight': 'bold',\n",
    "                    'size': 6\n",
    "                    }\n",
    "            for xx,yy in zip(pX,pY):\n",
    "                p += 1\n",
    "            #   place it below the lines\n",
    "                if xx in[30, 50, 100]:\n",
    "                    axes[c][j].text(xx-0.5,yy-7.5, ps[p], fontdict=font)    \n",
    "            #  place it above the lines\n",
    "                else:\n",
    "                    axes[c][j].text(xx-0.75,yy+4.5, ps[p], fontdict=font) \n",
    "#         --------------------------------\n",
    "\n",
    "\n",
    "axes[1][0].set_ylabel(f'{y_label} (with predicted frames)')\n",
    "axes[-1][c].set_xlabel(X_label)\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes[:1]]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "plt.figlegend( lines, labels, loc = 'lower center', ncol=3, labelspacing=0. , bbox_to_anchor=(0.5, -0.001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-plumbing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
